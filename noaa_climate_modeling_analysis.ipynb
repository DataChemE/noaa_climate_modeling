{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTSA 5741 Final Project: Modeling Climate Anomalies with Statistical Analysis\n",
    " \n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This Jupyter Notebook presents the final project for DTSA 5741, focused on analyzing climate anomalies using statistical methods. The primary objective of this project is to explore and analyze historical climate data for a selected region within the United States, leveraging publicly available datasets to uncover trends and anomalies in air temperature, precipitation, and groundwater levels.\n",
    "\n",
    "For this analysis, I selected Minnesota as the study area. This region is notable for its cold weather and diverse climate patterns, making it an interesting case study for climate analysis. By examining historical climate data for Minnesota, we aim to identify any significant anomalies or trends that may have occurred over the past few decades.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "1. **[NOAA Climate Data Online Portal](https://www.ncei.noaa.gov/cdo-web/):**\n",
    "   - Provides access to a wide range of historical weather and climate data from observation stations across the United States.\n",
    "   - Used to retrieve data on air temperature and precipitation for the selected region.\n",
    "\n",
    "2. **[USGS National Water Dashboard](https://dashboard.waterdata.usgs.gov/):**\n",
    "   - Offers real-time data and trends related to water resources such as groundwater levels and streamflow.\n",
    "   - Supplemented NOAA data with hydrological information for the selected region.\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "The goals of this analysis include:  \n",
    "1. **Data Acquisition:** Importing and preprocessing datasets to ensure consistency and usability.  \n",
    "2. **Exploratory Data Analysis (EDA):** Understanding historical trends in climate variables and identifying any anomalies.  \n",
    "3. **Statistical Analysis:** Performing in-depth analysis to interpret patterns in air temperature, precipitation, and groundwater levels.  \n",
    "4. **Visualization:** Presenting findings through clear and insightful visualizations to support conclusions.  \n",
    "5. **Reporting:** Documenting the process and results in a structured and reproducible manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Structure\n",
    "\n",
    "1. **Data Import and Cleaning:**  \n",
    "   - Importing datasets \n",
    "   - Cleaning and wrangling data to address missing values, inconsistencies, or outliers.\n",
    "\n",
    "2. **Exploratory Data Analysis:**  \n",
    "   - Visualizing historical trends in air temperature, precipitation, and groundwater levels.  \n",
    "   - Identifying seasonal patterns and anomalies.\n",
    "\n",
    "3. **Statistical Analysis:**  \n",
    "   - Applying statistical methods to investigate climate anomalies.  \n",
    "   - Comparing long-term trends across the selected time periods.\n",
    "\n",
    "4. **Results and Discussion:**  \n",
    "   - Highlighting significant findings from the analysis.  \n",
    "   - Discussing the implications of identified climate anomalies on the selected region.\n",
    "\n",
    "5. **Conclusion:**  \n",
    "   - Summarizing key insights and potential areas for further study.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Project Imports\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import gzip\n",
    "import shutil\n",
    "import datetime\n",
    "import time \n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NWIS API Call Import\n",
    "import dataretrieval.nwis as nwis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. **Data Import and Cleaning:**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Global Historical Climatology Network-Daily (GHCN-Daily) is an extensive database that compiles daily climate records from over 100,000 land-based stations worldwide. It encompasses more than 40 meteorological elements, including daily maximum and minimum temperatures, precipitation, snowfall, snow depth, evaporation, wind movement, soil temperature, and cloudiness. The dataset aggregates approximately 1.4 billion data values, with records dating back to the 19th century. Where feasible, station records are updated daily and are typically accessible one to two days after observation.\n",
    "\n",
    "Users can access the data in various formats:\n",
    "\t1.\tGHCN-Daily Form (PDF): Provides five core values and, when available, additional elements such as temperature at observation time, evaporation, 24-hour wind movement, and soil temperatures. Units are presented in either metric or standard, based on user preference.\n",
    "\t2.\tCustom GHCN-Daily CSV: Offers data optimized for spreadsheet use, allowing users to select preferred units, include flags, station names, geographic locations, and specify desired elements.\n",
    "\t3.\tCustom GHCN-Daily ASCII: Delivers data in ASCII text format with options similar to the CSV format, enabling customization of included information.\n",
    "\n",
    "The GHCN-Daily dataset serves as a replacement for older datasets maintained by the National Climatic Data Center (NCDC) and functions as the official archive for daily data from the Global Climate Observing System (GCOS) Surface Network. It is particularly suited for monitoring and assessing the frequency and magnitude of climate extremes.\n",
    "\n",
    "Some data are sourced under the World Meteorological Organization’s World Weather Watch Program, adhering to WMO Resolution 40 (Cg-XII). This permits member countries to impose restrictions on the commercial use or re-export of their data outside the receiving country. Consequently, data summaries and products are intended for unrestricted use in research, education, and other non-commercial activities. For non-U.S. locations, data or any derived products should not be provided to other users or used for the re-export of commercial services.\n",
    "\n",
    "For detailed information on data formats, observation definitions, and access methods, please refer to the GHCN-Daily documentation. ￼\n",
    "\n",
    "https://www.ncei.noaa.gov/cdo-web/datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/\n",
    "\n",
    "\n",
    "The \"station\".csv files contain all daily elements for that GHCN station for its entire period of record. \n",
    "Each element-day is provided on a separate line and all files are updated daily for the entire period of record.\n",
    "\n",
    "The following information serves as a definition of each field for all element-day records. \n",
    "Each field described below is separated by a comma ( , ) and follows the order below:\n",
    "\n",
    "ID = 11 character station identification code\n",
    "YEAR/MONTH/DAY = 8 character date in YYYYMMDD format (e.g. 19860529 = May 29, 1986)\n",
    "ELEMENT = 4 character indicator of element type \n",
    "DATA VALUE = 5 character data value for ELEMENT \n",
    "M-FLAG = 1 character Measurement Flag \n",
    "Q-FLAG = 1 character Quality Flag \n",
    "S-FLAG = 1 character Source Flag \n",
    "OBS-TIME = 4-character time of observation in hour-minute format (i.e. 0700 =7:00 am); if no ob time information \n",
    "is available, the field is left empty\n",
    "\n",
    "See section III of the GHCN-Daily readme.txt file (ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt)\n",
    "for an explanation of ELEMENT codes and their units as well as the M-FLAG, Q-FLAG and S-FLAG.\n",
    "\n",
    "The OBS-TIME field is populated with the observation times contained in NOAA/NCEI's HOMR station history database.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the data from the following link: https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOAA Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL for the dataset\n",
    "BASE_URL = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/\"\n",
    "\n",
    "# Directory to save files\n",
    "DATA_DIR = \"ghcn_us_data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Output file\n",
    "OUTPUT_FILE = \"ghcn_daily_combined.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the html of the webpage to find the links to the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML Response:\n",
      "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\n",
      "<html>\n",
      " <head>\n",
      "  <title>Index of /pub/data/ghcn/daily/by_station</title>\n",
      " </head>\n",
      " <body>\n",
      "<h1>Index of /pub/data/ghcn/daily/by_station</h1>\n",
      "  <table>\n",
      "   <tr><th><a href=\"?C=N;O=D\">Name</a></th><th><a href=\"?C=M;O=A\">Last modified</a></th><th><a href=\"?C=S;O=A\">Size</a></th><th><a href=\"?C=D;O=A\">Description</a></th></tr>\n",
      "   <tr><th colspan=\"4\"><hr></th></tr>\n",
      "<tr><td><a href=\"/pub/data/ghcn/daily/\">Parent Directory</a></td><td>&nbsp;</td><td align=\"right\">  - </td><td>&nbsp;</td></tr>\n",
      "<tr><td><a href=\"ACW00011604.csv.gz\">ACW00011604.csv.gz</a></td><td align=\"right\">2024-12-08 07:28  </td><td align=\"right\">3.9K</td><td>&nbsp;</td></tr>\n",
      "<tr><td><a href=\"ACW00011647.csv.gz\">ACW00011647.csv.gz</a></td><td align=\"right\">2024-12-08 07:28  </td><td align=\"right\"> 39K</td><td>&nbsp;</td></tr>\n",
      "<tr><td><a href=\"AE000041196.csv.gz\">AE000041196.csv.gz</a></td><td align=\"right\">2024-12-08 07:28  </td><td align=\"right\">250K</td><td>&nbsp;</td></\n"
     ]
    }
   ],
   "source": [
    "# Find the correct html references for the requests to download the data\n",
    "def fetch_and_inspect():\n",
    "    response = requests.get(BASE_URL)\n",
    "    response.raise_for_status()\n",
    "    print(\"HTML Response:\")\n",
    "    print(response.text[:1000]) \n",
    "\n",
    "fetch_and_inspect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on above the data is in the  'a' tag and the data is in the form of csv files. The data is extracted from the csv files and the data is cleaned and stored in the form of dataframes. The data is then used for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for downloading the daily NOAA data for the US sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worker function to download and extract a single file\n",
    "def process_file(us_file):\n",
    "    file_url = BASE_URL + us_file\n",
    "    file_path = os.path.join(DATA_DIR, us_file)\n",
    "    extracted_path = file_path.replace(\".gz\", \"\")\n",
    "\n",
    "    # Download the file if not already downloaded\n",
    "    if not os.path.exists(file_path):\n",
    "        try:\n",
    "            response = requests.get(file_url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            with open(file_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "        except Exception as e:\n",
    "            return f\"Error downloading {file_url}: {e}\"\n",
    "\n",
    "    # Extract the file\n",
    "    if not os.path.exists(extracted_path):\n",
    "        try:\n",
    "            with gzip.open(file_path, 'rb') as f_in:\n",
    "                with open(extracted_path, 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "        except Exception as e:\n",
    "            return f\"Error extracting {file_path}: {e}\"\n",
    "\n",
    "    return extracted_path\n",
    "\n",
    "\n",
    "# Download and extract files with parallelization\n",
    "def download_and_extract_us_files():\n",
    "    # Fetch the directory listing\n",
    "    response = requests.get(BASE_URL)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Parse the HTML response\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all anchor tags\n",
    "    links = soup.find_all('a')\n",
    "\n",
    "    # Extract and filter links ending with \".csv.gz\"\n",
    "    us_files = [link.get('href') for link in links if link.get('href') and link.get('href').startswith('US') and link.get('href').endswith('.csv.gz')]\n",
    "\n",
    "    print(f\"Found {len(us_files)} files to process.\")\n",
    "\n",
    "    extracted_files = []\n",
    "    errors = []\n",
    "\n",
    "    # Use ThreadPoolExecutor for parallel processing\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:  # Adjust max_workers based on your CPU\n",
    "        futures = {executor.submit(process_file, us_file): us_file for us_file in us_files}\n",
    "\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing files\", unit=\"file\"):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if isinstance(result, str) and result.startswith(\"Error\"):\n",
    "                    errors.append(result)\n",
    "                else:\n",
    "                    extracted_files.append(result)\n",
    "            except Exception as e:\n",
    "                errors.append(f\"Unexpected error: {e}\")\n",
    "\n",
    "    print(f\"Processed {len(extracted_files)} files successfully.\")\n",
    "    print(f\"Encountered {len(errors)} errors.\")\n",
    "    \n",
    "    # Log errors if any\n",
    "    if errors:\n",
    "        with open(\"error_log.txt\", \"w\") as log_file:\n",
    "            for error in errors:\n",
    "                log_file.write(error + \"\\n\")\n",
    "        print(\"Errors logged to 'error_log.txt'.\")\n",
    "\n",
    "    return extracted_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for consolidating data from multiple files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to consolidate data into a single CSV\n",
    "def consolidate_to_csv(files, output_file):\n",
    "    combined_data = []\n",
    "\n",
    "    # Use tqdm for an overall progress bar\n",
    "    with tqdm(total=len(files), desc=\"Consolidating files\", unit=\"file\") as pbar:\n",
    "        for file in files:\n",
    "            try:\n",
    "                # Read data into a DataFrame\n",
    "                data = pd.read_csv(\n",
    "                    file, header=None,\n",
    "                    names=[\"ID\", \"DATE\", \"ELEMENT\", \"DATA_VALUE\", \"M_FLAG\", \"Q_FLAG\", \"S_FLAG\", \"OBS_TIME\"]\n",
    "                )\n",
    "                combined_data.append(data)\n",
    "                pbar.set_postfix({\"Last File\": file.split(\"/\")[-1]})  # Show the last processed file\n",
    "            except Exception as e:\n",
    "                tqdm.write(f\"Error processing {file}: {e}\")\n",
    "            finally:\n",
    "                # Update the progress bar for each file\n",
    "                pbar.update(1)\n",
    "\n",
    "    # Concatenate all data and save to a single CSV file\n",
    "    if combined_data:\n",
    "        combined_df = pd.concat(combined_data, ignore_index=True)\n",
    "        combined_df.to_csv(output_file, index=False)\n",
    "        print(f\"All data consolidated into {output_file}.\")\n",
    "    else:\n",
    "        print(\"No data to consolidate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and Extract Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and extracting files...\n",
      "Found 74429 files to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  76%|███████▋  | 56766/74429 [15:30<16:22, 17.97file/s]   "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Download and extract files\n",
    "    print(\"Downloading and extracting files...\")\n",
    "    extracted_files = download_and_extract_us_files()\n",
    "\n",
    "    # Save the list of extracted files for the next step\n",
    "    with open(\"extracted_files.txt\", \"w\") as f:\n",
    "        for file in extracted_files:\n",
    "            f.write(file + \"\\n\")\n",
    "\n",
    "    print(f\"Extracted file paths saved to 'extracted_files.txt'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consolidate Data into a Single CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Step 2: Consolidate data into a single CSV\n",
    "    print(\"Reading extracted file paths...\")\n",
    "    try:\n",
    "        with open(\"extracted_files.txt\", \"r\") as f:\n",
    "            extracted_files = [line.strip() for line in f.readlines()]\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'extracted_files.txt' not found. Please run Step 1 first.\")\n",
    "        exit(1)\n",
    "\n",
    "    print(f\"Found {len(extracted_files)} files to consolidate.\")\n",
    "    print(\"Consolidating data into a single CSV file...\")\n",
    "\n",
    "    # Call the function with the tqdm bar\n",
    "    consolidate_to_csv(extracted_files, OUTPUT_FILE)\n",
    "\n",
    "    print(f\"Data consolidated into '{OUTPUT_FILE}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyrdometeorological data from USGS \n",
    "\n",
    "https://github.com/DOI-USGS/dataretrieval-python \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mississippi River at St. Paul MN (USGS 05331000)\n",
    "# site = '05331000'\n",
    "# start = '2019-01-01'\n",
    "# end = '2019-12-31'\n",
    "\n",
    "# # Get the data\n",
    "# hydro_data = nwis.get_record(sites=site, service='stat', start=start, end=end)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. **Exploratory Data Analysis:**  \n",
    "\n",
    "- Air temperature\n",
    "- Precipitation\n",
    "- Groundwater level \n",
    "\n",
    "   - Visualizing historical trends in air temperature, precipitation, and groundwater levels.  \n",
    "   - Identifying seasonal patterns and anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. **Statistical Analysis:** \n",
    "\n",
    "\n",
    "   - Applying statistical methods to investigate climate anomalies.  \n",
    "   - Comparing long-term trends across the selected time periods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. **Results and Discussion:**  \n",
    "\n",
    "   - Highlighting significant findings from the analysis.  \n",
    "   - Discussing the implications of identified climate anomalies on the selected region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. **Conclusion:**  \n",
    "   - Summarizing key insights and potential areas for further study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
