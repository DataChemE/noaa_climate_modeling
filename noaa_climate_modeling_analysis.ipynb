{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTSA 5741 Final Project: Modeling Climate Anomalies with Statistical Analysis\n",
    " \n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This Jupyter Notebook presents the final project for DTSA 5741, focused on analyzing climate anomalies using statistical methods. The primary objective of this project is to explore and analyze historical climate data for a selected region within the United States, leveraging publicly available datasets to uncover trends and anomalies in air temperature, precipitation, and groundwater levels.\n",
    "\n",
    "For this analysis, I selected Minnesota as the study area. This region is notable for its cold weather and diverse climate patterns, making it an interesting case study for climate analysis. By examining historical climate data for Minnesota, we aim to identify any significant anomalies or trends that may have occurred over the past few decades.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "1. **[NOAA Climate Data Online Portal](https://www.ncei.noaa.gov/cdo-web/):**\n",
    "   - Provides access to a wide range of historical weather and climate data from observation stations across the United States.\n",
    "   - Used to retrieve data on air temperature and precipitation for the selected region.\n",
    "\n",
    "2. **[USGS National Water Dashboard](https://dashboard.waterdata.usgs.gov/):**\n",
    "   - Offers real-time data and trends related to water resources such as groundwater levels and streamflow.\n",
    "   - Supplemented NOAA data with hydrological information for the selected region.\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "The goals of this analysis include:  \n",
    "1. **Data Acquisition:** Importing and preprocessing datasets to ensure consistency and usability.  \n",
    "2. **Exploratory Data Analysis (EDA):** Understanding historical trends in climate variables and identifying any anomalies.  \n",
    "3. **Statistical Analysis:** Performing in-depth analysis to interpret patterns in air temperature, precipitation, and groundwater levels.  \n",
    "4. **Visualization:** Presenting findings through clear and insightful visualizations to support conclusions.  \n",
    "5. **Reporting:** Documenting the process and results in a structured and reproducible manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Structure\n",
    "\n",
    "1. **Data Import and Cleaning:**  \n",
    "   - Importing datasets \n",
    "   - Cleaning and wrangling data to address missing values, inconsistencies, or outliers.\n",
    "\n",
    "2. **Exploratory Data Analysis:**  \n",
    "   - Visualizing historical trends in air temperature, precipitation, and groundwater levels.  \n",
    "   - Identifying seasonal patterns and anomalies.\n",
    "\n",
    "3. **Statistical Analysis:**  \n",
    "   - Applying statistical methods to investigate climate anomalies.  \n",
    "   - Comparing long-term trends across the selected time periods.\n",
    "\n",
    "4. **Results and Discussion:**  \n",
    "   - Highlighting significant findings from the analysis.  \n",
    "   - Discussing the implications of identified climate anomalies on the selected region.\n",
    "\n",
    "5. **Conclusion:**  \n",
    "   - Summarizing key insights and potential areas for further study.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Project Imports\n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pdfplumber\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NWIS API Call Import\n",
    "import dataretrieval.nwis as nwis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. **Data Import and Cleaning:**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Global Historical Climatology Network-Daily (GHCN-Daily) is an extensive database that compiles daily climate records from over 100,000 land-based stations worldwide. It encompasses more than 40 meteorological elements, including daily maximum and minimum temperatures, precipitation, snowfall, snow depth, evaporation, wind movement, soil temperature, and cloudiness. The dataset aggregates approximately 1.4 billion data values, with records dating back to the 19th century. Where feasible, station records are updated daily and are typically accessible one to two days after observation.\n",
    "\n",
    "Users can access the data in various formats:\n",
    "\t1.\tGHCN-Daily Form (PDF): Provides five core values and, when available, additional elements such as temperature at observation time, evaporation, 24-hour wind movement, and soil temperatures. Units are presented in either metric or standard, based on user preference.\n",
    "\t2.\tCustom GHCN-Daily CSV: Offers data optimized for spreadsheet use, allowing users to select preferred units, include flags, station names, geographic locations, and specify desired elements.\n",
    "\t3.\tCustom GHCN-Daily ASCII: Delivers data in ASCII text format with options similar to the CSV format, enabling customization of included information.\n",
    "\n",
    "The GHCN-Daily dataset serves as a replacement for older datasets maintained by the National Climatic Data Center (NCDC) and functions as the official archive for daily data from the Global Climate Observing System (GCOS) Surface Network. It is particularly suited for monitoring and assessing the frequency and magnitude of climate extremes.\n",
    "\n",
    "Some data are sourced under the World Meteorological Organization’s World Weather Watch Program, adhering to WMO Resolution 40 (Cg-XII). This permits member countries to impose restrictions on the commercial use or re-export of their data outside the receiving country. Consequently, data summaries and products are intended for unrestricted use in research, education, and other non-commercial activities. For non-U.S. locations, data or any derived products should not be provided to other users or used for the re-export of commercial services.\n",
    "\n",
    "For detailed information on data formats, observation definitions, and access methods, please refer to the GHCN-Daily documentation. ￼\n",
    "\n",
    "https://www.ncei.noaa.gov/cdo-web/datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/\n",
    "\n",
    "\n",
    "The \"station\".csv files contain all daily elements for that GHCN station for its entire period of record. \n",
    "Each element-day is provided on a separate line and all files are updated daily for the entire period of record.\n",
    "\n",
    "The following information serves as a definition of each field for all element-day records. \n",
    "Each field described below is separated by a comma ( , ) and follows the order below:\n",
    "\n",
    "ID = 11 character station identification code\n",
    "YEAR/MONTH/DAY = 8 character date in YYYYMMDD format (e.g. 19860529 = May 29, 1986)\n",
    "ELEMENT = 4 character indicator of element type \n",
    "DATA VALUE = 5 character data value for ELEMENT \n",
    "M-FLAG = 1 character Measurement Flag \n",
    "Q-FLAG = 1 character Quality Flag \n",
    "S-FLAG = 1 character Source Flag \n",
    "OBS-TIME = 4-character time of observation in hour-minute format (i.e. 0700 =7:00 am); if no ob time information \n",
    "is available, the field is left empty\n",
    "\n",
    "See section III of the GHCN-Daily readme.txt file (ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt)\n",
    "for an explanation of ELEMENT codes and their units as well as the M-FLAG, Q-FLAG and S-FLAG.\n",
    "\n",
    "The OBS-TIME field is populated with the observation times contained in NOAA/NCEI's HOMR station history database.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the data from the following link: https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First NOAA Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL for the dataset\n",
    "BASE_URL = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/\"\n",
    "\n",
    "# Directory to save files\n",
    "DATA_DIR = \"ghcn_us_data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Output file\n",
    "OUTPUT_FILE = \"ghcn_daily_combined.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract_us_files():\n",
    "    # Fetch the directory listing\n",
    "    response = requests.get(BASE_URL)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Parse the HTML response\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all anchor tags\n",
    "    links = soup.find_all('a')\n",
    "\n",
    "    # Extract and filter links ending with \".csv.gz\"\n",
    "    us_files = [link.get('href') for link in links if link.get('href') and link.get('href').startswith('US') and link.get('href').endswith('.csv.gz')]\n",
    "\n",
    "    print(f\"Found {len(us_files)} files to download.\")\n",
    "    \n",
    "    extracted_files = []\n",
    "    for us_file in us_files:\n",
    "        # Full URL for the file\n",
    "        file_url = BASE_URL + us_file\n",
    "        file_path = os.path.join(DATA_DIR, us_file)\n",
    "        extracted_path = file_path.replace(\".gz\", \"\")\n",
    "\n",
    "        print(f\"Processing file: {file_url}\")\n",
    "\n",
    "        # Download the file if not already downloaded\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Downloading {file_url}\")\n",
    "            response = requests.get(file_url)\n",
    "            response.raise_for_status()\n",
    "            with open(file_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "        \n",
    "        # Extract the file\n",
    "        if not os.path.exists(extracted_path):\n",
    "            print(f\"Extracting {file_path}\")\n",
    "            with gzip.open(file_path, 'rb') as f_in:\n",
    "                with open(extracted_path, 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "        \n",
    "        extracted_files.append(extracted_path)\n",
    "    \n",
    "    print(f\"Extracted {len(extracted_files)} files.\")\n",
    "    return extracted_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to consolidate data into a single CSV\n",
    "def consolidate_to_csv(files, output_file):\n",
    "    combined_data = []\n",
    "    for file in files:\n",
    "        print(f\"Processing file: {file}\")\n",
    "        try:\n",
    "            # Read data into a DataFrame\n",
    "            data = pd.read_csv(\n",
    "                file, header=None,\n",
    "                names=[\"ID\", \"DATE\", \"ELEMENT\", \"DATA_VALUE\", \"M_FLAG\", \"Q_FLAG\", \"S_FLAG\", \"OBS_TIME\"]\n",
    "            )\n",
    "            print(f\"File {file} parsed successfully with {len(data)} rows.\")\n",
    "            combined_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "    \n",
    "    # Concatenate all data and save to a single CSV file\n",
    "    if combined_data:\n",
    "        combined_df = pd.concat(combined_data, ignore_index=True)\n",
    "        combined_df.to_csv(output_file, index=False)\n",
    "        print(f\"All data consolidated into {output_file}.\")\n",
    "    else:\n",
    "        print(\"No data to consolidate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and extracting files...\n"
     ]
    }
   ],
   "source": [
    "# Main execution of functions above to download and extract files\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Download and extract files\n",
    "    print(\"Downloading and extracting files...\")\n",
    "    extracted_files = download_and_extract_us_files()\n",
    "\n",
    "    # Step 2: Consolidate data into a single CSV\n",
    "    print(\"Consolidating data into a single CSV file...\")\n",
    "    consolidate_to_csv(extracted_files, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(DATA_DIR):\n",
    "    file_path = os.path.join(DATA_DIR, file)\n",
    "    if file_path.endswith(\".csv\"):\n",
    "        print(f\"Contents of {file}:\")\n",
    "        with open(file_path, \"r\") as f:\n",
    "            for _ in range(5):  # Print the first 5 lines\n",
    "                print(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML Response:\n",
      "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\n",
      "<html>\n",
      " <head>\n",
      "  <title>Index of /pub/data/ghcn/daily/by_station</title>\n",
      " </head>\n",
      " <body>\n",
      "<h1>Index of /pub/data/ghcn/daily/by_station</h1>\n",
      "  <table>\n",
      "   <tr><th><a href=\"?C=N;O=D\">Name</a></th><th><a href=\"?C=M;O=A\">Last modified</a></th><th><a href=\"?C=S;O=A\">Size</a></th><th><a href=\"?C=D;O=A\">Description</a></th></tr>\n",
      "   <tr><th colspan=\"4\"><hr></th></tr>\n",
      "<tr><td><a href=\"/pub/data/ghcn/daily/\">Parent Directory</a></td><td>&nbsp;</td><td align=\"right\">  - </td><td>&nbsp;</td></tr>\n",
      "<tr><td><a href=\"ACW00011604.csv.gz\">ACW00011604.csv.gz</a></td><td align=\"right\">2024-12-08 07:28  </td><td align=\"right\">3.9K</td><td>&nbsp;</td></tr>\n",
      "<tr><td><a href=\"ACW00011647.csv.gz\">ACW00011647.csv.gz</a></td><td align=\"right\">2024-12-08 07:28  </td><td align=\"right\"> 39K</td><td>&nbsp;</td></tr>\n",
      "<tr><td><a href=\"AE000041196.csv.gz\">AE000041196.csv.gz</a></td><td align=\"right\">2024-12-08 07:28  </td><td align=\"right\">250K</td><td>&nbsp;</td></\n"
     ]
    }
   ],
   "source": [
    "def fetch_and_inspect():\n",
    "    response = requests.get(BASE_URL)\n",
    "    response.raise_for_status()\n",
    "    print(\"HTML Response:\")\n",
    "    print(response.text[:1000])  # Print the first 1000 characters for inspection\n",
    "\n",
    "fetch_and_inspect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regional Data from NOAA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATION</th>\n",
       "      <th>NAME</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>ELEVATION</th>\n",
       "      <th>DATE</th>\n",
       "      <th>DAPR</th>\n",
       "      <th>DAPR_ATTRIBUTES</th>\n",
       "      <th>MDPR</th>\n",
       "      <th>MDPR_ATTRIBUTES</th>\n",
       "      <th>PRCP</th>\n",
       "      <th>PRCP_ATTRIBUTES</th>\n",
       "      <th>SNOW</th>\n",
       "      <th>SNOW_ATTRIBUTES</th>\n",
       "      <th>SNWD</th>\n",
       "      <th>SNWD_ATTRIBUTES</th>\n",
       "      <th>WESD</th>\n",
       "      <th>WESD_ATTRIBUTES</th>\n",
       "      <th>WESF</th>\n",
       "      <th>WESF_ATTRIBUTES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US1MNRM0108</td>\n",
       "      <td>ROSEVILLE 2.2 ENE, MN US</td>\n",
       "      <td>45.02814</td>\n",
       "      <td>-93.11325</td>\n",
       "      <td>280.7</td>\n",
       "      <td>2020-04-19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>,,N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>,,N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US1MNRM0108</td>\n",
       "      <td>ROSEVILLE 2.2 ENE, MN US</td>\n",
       "      <td>45.02814</td>\n",
       "      <td>-93.11325</td>\n",
       "      <td>280.7</td>\n",
       "      <td>2020-04-20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>,,N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>,,N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US1MNRM0108</td>\n",
       "      <td>ROSEVILLE 2.2 ENE, MN US</td>\n",
       "      <td>45.02814</td>\n",
       "      <td>-93.11325</td>\n",
       "      <td>280.7</td>\n",
       "      <td>2020-04-21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>T,,N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US1MNRM0108</td>\n",
       "      <td>ROSEVILLE 2.2 ENE, MN US</td>\n",
       "      <td>45.02814</td>\n",
       "      <td>-93.11325</td>\n",
       "      <td>280.7</td>\n",
       "      <td>2020-04-22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>,,N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>,,N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US1MNRM0108</td>\n",
       "      <td>ROSEVILLE 2.2 ENE, MN US</td>\n",
       "      <td>45.02814</td>\n",
       "      <td>-93.11325</td>\n",
       "      <td>280.7</td>\n",
       "      <td>2020-04-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>,,N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>,,N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       STATION                      NAME  LATITUDE  LONGITUDE  ELEVATION  \\\n",
       "0  US1MNRM0108  ROSEVILLE 2.2 ENE, MN US  45.02814  -93.11325      280.7   \n",
       "1  US1MNRM0108  ROSEVILLE 2.2 ENE, MN US  45.02814  -93.11325      280.7   \n",
       "2  US1MNRM0108  ROSEVILLE 2.2 ENE, MN US  45.02814  -93.11325      280.7   \n",
       "3  US1MNRM0108  ROSEVILLE 2.2 ENE, MN US  45.02814  -93.11325      280.7   \n",
       "4  US1MNRM0108  ROSEVILLE 2.2 ENE, MN US  45.02814  -93.11325      280.7   \n",
       "\n",
       "         DATE  DAPR DAPR_ATTRIBUTES  MDPR MDPR_ATTRIBUTES  PRCP  \\\n",
       "0  2020-04-19   NaN             NaN   NaN             NaN   0.0   \n",
       "1  2020-04-20   NaN             NaN   NaN             NaN   0.0   \n",
       "2  2020-04-21   NaN             NaN   NaN             NaN   0.0   \n",
       "3  2020-04-22   NaN             NaN   NaN             NaN   0.0   \n",
       "4  2020-04-23   NaN             NaN   NaN             NaN   0.0   \n",
       "\n",
       "  PRCP_ATTRIBUTES  SNOW SNOW_ATTRIBUTES  SNWD SNWD_ATTRIBUTES  WESD  \\\n",
       "0             ,,N   0.0             ,,N   NaN             NaN   NaN   \n",
       "1             ,,N   0.0             ,,N   NaN             NaN   NaN   \n",
       "2            T,,N   NaN             NaN   NaN             NaN   NaN   \n",
       "3             ,,N   0.0             ,,N   NaN             NaN   NaN   \n",
       "4             ,,N   0.0             ,,N   NaN             NaN   NaN   \n",
       "\n",
       "  WESD_ATTRIBUTES  WESF WESF_ATTRIBUTES  \n",
       "0             NaN   NaN             NaN  \n",
       "1             NaN   NaN             NaN  \n",
       "2             NaN   NaN             NaN  \n",
       "3             NaN   NaN             NaN  \n",
       "4             NaN   NaN             NaN  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precip_data = pd.read_csv('./precip_data.csv')\n",
    "precip_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['STATION', 'NAME', 'LATITUDE', 'LONGITUDE', 'ELEVATION', 'DATE', 'DAPR',\n",
       "       'DAPR_ATTRIBUTES', 'MDPR', 'MDPR_ATTRIBUTES', 'PRCP', 'PRCP_ATTRIBUTES',\n",
       "       'SNOW', 'SNOW_ATTRIBUTES', 'SNWD', 'SNWD_ATTRIBUTES', 'WESD',\n",
       "       'WESD_ATTRIBUTES', 'WESF', 'WESF_ATTRIBUTES'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precip_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the directory containing the .dly files and output CSV\n",
    "# # input_folder = './ghcnd_hcn'\n",
    "\n",
    "# input_folder = './test'\n",
    "# output_csv = \"daily_data.csv\"\n",
    "\n",
    "# # Initialize an empty list to collect all data\n",
    "# all_data = []\n",
    "\n",
    "# # Process each .dly file in the folder\n",
    "# for file_name in os.listdir(input_folder):\n",
    "#     if file_name.endswith(\".dly\"):\n",
    "#         file_path = os.path.join(input_folder, file_name)\n",
    "#         with open(file_path, \"r\") as file:\n",
    "#             for line in file:\n",
    "#                 # Split the line into chunks of fixed width (8 characters, as an example)\n",
    "#                 row = [line[i:i+8].strip() for i in range(0, len(line), 8)]\n",
    "#                 all_data.append(row)\n",
    "\n",
    "# # Determine the maximum number of columns dynamically\n",
    "# max_columns = max(len(row) for row in all_data)\n",
    "\n",
    "# # Create generic column names\n",
    "# columns = [f\"Column_{i+1}\" for i in range(max_columns)]\n",
    "\n",
    "# # Create a DataFrame and save to a CSV\n",
    "# df = pd.DataFrame(all_data, columns=columns)\n",
    "# df.to_csv(output_csv, index=False)\n",
    "\n",
    "# print(f\"Data successfully extracted and saved to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('daily_data.csv')\n",
    "\n",
    "# data['Column_3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyrdometeorological data from USGS \n",
    "\n",
    "https://github.com/DOI-USGS/dataretrieval-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mississippi River at St. Paul MN (USGS 05331000)\n",
    "site = '05331000'\n",
    "start = '2019-01-01'\n",
    "end = '2019-12-31'\n",
    "\n",
    "# Get the data\n",
    "hydro_data = nwis.get_record(sites=site, service='stat', start=start, end=end)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. **Exploratory Data Analysis:**  \n",
    "\n",
    "- Air temperature\n",
    "- Precipitation\n",
    "- Groundwater level \n",
    "\n",
    "   - Visualizing historical trends in air temperature, precipitation, and groundwater levels.  \n",
    "   - Identifying seasonal patterns and anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. **Statistical Analysis:** \n",
    "\n",
    "\n",
    "   - Applying statistical methods to investigate climate anomalies.  \n",
    "   - Comparing long-term trends across the selected time periods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. **Results and Discussion:**  \n",
    "\n",
    "   - Highlighting significant findings from the analysis.  \n",
    "   - Discussing the implications of identified climate anomalies on the selected region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. **Conclusion:**  \n",
    "   - Summarizing key insights and potential areas for further study."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
