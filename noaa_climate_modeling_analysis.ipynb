{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTSA 5741 Final Project: Modeling Climate Anomalies with Statistical Analysis\n",
    " \n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This Jupyter Notebook presents the final project for DTSA 5741, focused on analyzing climate anomalies using statistical methods. The primary objective of this project is to explore and analyze historical climate data for a selected region within the United States, leveraging publicly available datasets to uncover trends and anomalies in air temperature, precipitation, and groundwater levels.\n",
    "\n",
    "For this analysis, I selected Minnesota as the study area. This region is notable for its cold weather and diverse climate patterns, making it an interesting case study for climate analysis. By examining historical climate data for Minnesota, we aim to identify any significant anomalies or trends that may have occurred over the past few decades.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "1. **[NOAA Climate Data Online Portal](https://www.ncei.noaa.gov/cdo-web/):**\n",
    "   - Provides access to a wide range of historical weather and climate data from observation stations across the United States.\n",
    "   - Used to retrieve data on air temperature and precipitation for the selected region.\n",
    "\n",
    "2. **[USGS National Water Dashboard](https://dashboard.waterdata.usgs.gov/):**\n",
    "   - Offers real-time data and trends related to water resources such as groundwater levels and streamflow.\n",
    "   - Supplemented NOAA data with hydrological information for the selected region.\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "The goals of this analysis include:  \n",
    "1. **Data Acquisition:** Importing and preprocessing datasets to ensure consistency and usability.  \n",
    "2. **Exploratory Data Analysis (EDA):** Understanding historical trends in climate variables and identifying any anomalies.  \n",
    "3. **Statistical Analysis:** Performing in-depth analysis to interpret patterns in air temperature, precipitation, and groundwater levels.  \n",
    "4. **Visualization:** Presenting findings through clear and insightful visualizations to support conclusions.  \n",
    "5. **Reporting:** Documenting the process and results in a structured and reproducible manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Structure\n",
    "\n",
    "1. **Data Import and Cleaning:**  \n",
    "   - Importing datasets \n",
    "   - Cleaning and wrangling data to address missing values, inconsistencies, or outliers.\n",
    "\n",
    "2. **Exploratory Data Analysis:**  \n",
    "   - Visualizing historical trends in air temperature, precipitation, and groundwater levels.  \n",
    "   - Identifying seasonal patterns and anomalies.\n",
    "\n",
    "3. **Statistical Analysis:**  \n",
    "   - Applying statistical methods to investigate climate anomalies.  \n",
    "   - Comparing long-term trends across the selected time periods.\n",
    "\n",
    "4. **Results and Discussion:**  \n",
    "   - Highlighting significant findings from the analysis.  \n",
    "   - Discussing the implications of identified climate anomalies on the selected region.\n",
    "\n",
    "5. **Conclusion:**  \n",
    "   - Summarizing key insights and potential areas for further study.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Project Imports\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import gzip\n",
    "import shutil\n",
    "import datetime\n",
    "import time \n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# NWIS API Call Import\n",
    "import dataretrieval.nwis as nwis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. **Data Import and Cleaning:**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Global Historical Climatology Network-Daily (GHCN-Daily) is an extensive database that compiles daily climate records from over 100,000 land-based stations worldwide. It encompasses more than 40 meteorological elements, including daily maximum and minimum temperatures, precipitation, snowfall, snow depth, evaporation, wind movement, soil temperature, and cloudiness. The dataset aggregates approximately 1.4 billion data values, with records dating back to the 19th century. Where feasible, station records are updated daily and are typically accessible one to two days after observation.\n",
    "\n",
    "Users can access the data in various formats:\n",
    "\t1.\tGHCN-Daily Form (PDF): Provides five core values and, when available, additional elements such as temperature at observation time, evaporation, 24-hour wind movement, and soil temperatures. Units are presented in either metric or standard, based on user preference.\n",
    "\t2.\tCustom GHCN-Daily CSV: Offers data optimized for spreadsheet use, allowing users to select preferred units, include flags, station names, geographic locations, and specify desired elements.\n",
    "\t3.\tCustom GHCN-Daily ASCII: Delivers data in ASCII text format with options similar to the CSV format, enabling customization of included information.\n",
    "\n",
    "The GHCN-Daily dataset serves as a replacement for older datasets maintained by the National Climatic Data Center (NCDC) and functions as the official archive for daily data from the Global Climate Observing System (GCOS) Surface Network. It is particularly suited for monitoring and assessing the frequency and magnitude of climate extremes.\n",
    "\n",
    "Some data are sourced under the World Meteorological Organization’s World Weather Watch Program, adhering to WMO Resolution 40 (Cg-XII). This permits member countries to impose restrictions on the commercial use or re-export of their data outside the receiving country. Consequently, data summaries and products are intended for unrestricted use in research, education, and other non-commercial activities. For non-U.S. locations, data or any derived products should not be provided to other users or used for the re-export of commercial services.\n",
    "\n",
    "For detailed information on data formats, observation definitions, and access methods, please refer to the GHCN-Daily documentation. ￼\n",
    "\n",
    "https://www.ncei.noaa.gov/cdo-web/datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/\n",
    "\n",
    "\n",
    "The \"station\".csv files contain all daily elements for that GHCN station for its entire period of record. \n",
    "Each element-day is provided on a separate line and all files are updated daily for the entire period of record.\n",
    "\n",
    "The following information serves as a definition of each field for all element-day records. \n",
    "Each field described below is separated by a comma ( , ) and follows the order below:\n",
    "\n",
    "ID = 11 character station identification code\n",
    "YEAR/MONTH/DAY = 8 character date in YYYYMMDD format (e.g. 19860529 = May 29, 1986)\n",
    "ELEMENT = 4 character indicator of element type \n",
    "DATA VALUE = 5 character data value for ELEMENT \n",
    "M-FLAG = 1 character Measurement Flag \n",
    "Q-FLAG = 1 character Quality Flag \n",
    "S-FLAG = 1 character Source Flag \n",
    "OBS-TIME = 4-character time of observation in hour-minute format (i.e. 0700 =7:00 am); if no ob time information \n",
    "is available, the field is left empty\n",
    "\n",
    "See section III of the GHCN-Daily readme.txt file (ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt)\n",
    "for an explanation of ELEMENT codes and their units as well as the M-FLAG, Q-FLAG and S-FLAG.\n",
    "\n",
    "The OBS-TIME field is populated with the observation times contained in NOAA/NCEI's HOMR station history database.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the data from the following link: https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/\n",
    "\n",
    "Station info: \n",
    "\n",
    "https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOAA Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL for the dataset\n",
    "BASE_URL = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/\"\n",
    "\n",
    "# Directory to save files\n",
    "DATA_DIR = \"ghcn_us_data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Output file\n",
    "OUTPUT_FILE = \"ghcn_daily_combined.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the html of the webpage to find the links to the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML Response:\n",
      "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\n",
      "<html>\n",
      " <head>\n",
      "  <title>Index of /pub/data/ghcn/daily/by_station</title>\n",
      " </head>\n",
      " <body>\n",
      "<h1>Index of /pub/data/ghcn/daily/by_station</h1>\n",
      "  <table>\n",
      "   <tr><th><a href=\"?C=N;O=D\">Name</a></th><th><a href=\"?C=M;O=A\">Last modified</a></th><th><a href=\"?C=S;O=A\">Size</a></th><th><a href=\"?C=D;O=A\">Description</a></th></tr>\n",
      "   <tr><th colspan=\"4\"><hr></th></tr>\n",
      "<tr><td><a href=\"/pub/data/ghcn/daily/\">Parent Directory</a></td><td>&nbsp;</td><td align=\"right\">  - </td><td>&nbsp;</td></tr>\n",
      "<tr><td><a href=\"ACW00011604.csv.gz\">ACW00011604.csv.gz</a></td><td align=\"right\">2024-12-09 07:28  </td><td align=\"right\">3.9K</td><td>&nbsp;</td></tr>\n",
      "<tr><td><a href=\"ACW00011647.csv.gz\">ACW00011647.csv.gz</a></td><td align=\"right\">2024-12-09 07:28  </td><td align=\"right\"> 39K</td><td>&nbsp;</td></tr>\n",
      "<tr><td><a href=\"AE000041196.csv.gz\">AE000041196.csv.gz</a></td><td align=\"right\">2024-12-09 07:28  </td><td align=\"right\">250K</td><td>&nbsp;</td></\n"
     ]
    }
   ],
   "source": [
    "# Find the correct html references for the requests to download the data\n",
    "def fetch_and_inspect():\n",
    "    response = requests.get(BASE_URL)\n",
    "    response.raise_for_status()\n",
    "    print(\"HTML Response:\")\n",
    "    print(response.text[:1000]) \n",
    "\n",
    "fetch_and_inspect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on above the data is in the  'a' tag and the data is in the form of csv files. The data is extracted from the csv files and the data is cleaned and stored in the form of dataframes. The data is then used for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for downloading the daily NOAA data for the US sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worker function to download and extract a single file\n",
    "def process_file(us_file):\n",
    "    file_url = BASE_URL + us_file\n",
    "    file_path = os.path.join(DATA_DIR, us_file)\n",
    "    extracted_path = file_path.replace(\".gz\", \"\")\n",
    "\n",
    "    # Download the file if not already downloaded\n",
    "    if not os.path.exists(file_path):\n",
    "        try:\n",
    "            response = requests.get(file_url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            with open(file_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "        except Exception as e:\n",
    "            return f\"Error downloading {file_url}: {e}\"\n",
    "\n",
    "    # Extract the file\n",
    "    if not os.path.exists(extracted_path):\n",
    "        try:\n",
    "            with gzip.open(file_path, 'rb') as f_in:\n",
    "                with open(extracted_path, 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "        except Exception as e:\n",
    "            return f\"Error extracting {file_path}: {e}\"\n",
    "\n",
    "    return extracted_path\n",
    "\n",
    "\n",
    "# Download and extract files with parallelization\n",
    "def download_and_extract_us_files():\n",
    "    # Fetch the directory listing\n",
    "    response = requests.get(BASE_URL)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Parse the HTML response\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all anchor tags\n",
    "    links = soup.find_all('a')\n",
    "\n",
    "    # Extract and filter links ending with \".csv.gz\"\n",
    "    us_files = [link.get('href') for link in links if link.get('href') and link.get('href').startswith('US') and link.get('href').endswith('.csv.gz')]\n",
    "\n",
    "    print(f\"Found {len(us_files)} files to process.\")\n",
    "\n",
    "    extracted_files = []\n",
    "    errors = []\n",
    "\n",
    "    # Use ThreadPoolExecutor for parallel processing\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor: \n",
    "        futures = {executor.submit(process_file, us_file): us_file for us_file in us_files}\n",
    "\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing files\", unit=\"file\"):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if isinstance(result, str) and result.startswith(\"Error\"):\n",
    "                    errors.append(result)\n",
    "                else:\n",
    "                    extracted_files.append(result)\n",
    "            except Exception as e:\n",
    "                errors.append(f\"Unexpected error: {e}\")\n",
    "\n",
    "    print(f\"Processed {len(extracted_files)} files successfully.\")\n",
    "    print(f\"Encountered {len(errors)} errors.\")\n",
    "    \n",
    "    # Log errors \n",
    "    if errors:\n",
    "        with open(\"error_log.txt\", \"w\") as log_file:\n",
    "            for error in errors:\n",
    "                log_file.write(error + \"\\n\")\n",
    "        print(\"Errors logged to 'error_log.txt'.\")\n",
    "\n",
    "    return extracted_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for consolidating data from multiple files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the data in one csv file \n",
    "def consolidate_to_csv(files, output_file, batch_size=1000):\n",
    "    # Open the output file for writing\n",
    "    with open(output_file, \"w\") as f_out:\n",
    "        # Use tqdm for an overall progress bar\n",
    "        with tqdm(total=len(files), desc=\"Consolidating files\", unit=\"file\") as pbar:\n",
    "            for i in range(0, len(files), batch_size):\n",
    "                batch_files = files[i : i + batch_size]\n",
    "                combined_data = []\n",
    "                for file in batch_files:\n",
    "                    try:\n",
    "                        # Read data into a DataFrame\n",
    "                        data = pd.read_csv(\n",
    "                            file,\n",
    "                            header=None,\n",
    "                            names=[\"ID\", \"DATE\", \"ELEMENT\", \"DATA_VALUE\", \"M_FLAG\", \"Q_FLAG\", \"S_FLAG\", \"OBS_TIME\"],\n",
    "                            dtype={\n",
    "                                \"ID\": str,\n",
    "                                \"DATE\": str,\n",
    "                                \"ELEMENT\": str,\n",
    "                                \"DATA_VALUE\": \"float64\",\n",
    "                                \"M_FLAG\": str,\n",
    "                                \"Q_FLAG\": str,\n",
    "                                \"S_FLAG\": str,\n",
    "                                \"OBS_TIME\": str,\n",
    "                            },\n",
    "                        )\n",
    "                        combined_data.append(data)\n",
    "                    except Exception as e:\n",
    "                        pbar.write(f\"Error processing {file}: {e}\")\n",
    "                    finally:\n",
    "                        pbar.update(1)\n",
    "\n",
    "                # Concatenate and write the batch to the output file\n",
    "                if combined_data:\n",
    "                    batch_df = pd.concat(combined_data, ignore_index=True)\n",
    "                    batch_df.to_csv(f_out, index=False, header=(i == 0), mode=\"a\")  # Write header only for the first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and Extract File paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and extracting files...\n",
      "Found 74429 files to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 74429/74429 [00:00<00:00, 80099.21file/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 74429 files successfully.\n",
      "Encountered 0 errors.\n",
      "Extracted file paths saved to 'extracted_files.txt'.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Download and extract files\n",
    "    print(\"Downloading and extracting files...\")\n",
    "    extracted_files = download_and_extract_us_files()\n",
    "\n",
    "    # Save the list of extracted files to a text file\n",
    "    with open(\"extracted_files.txt\", \"w\") as f:\n",
    "        for file in extracted_files:\n",
    "            f.write(file + \"\\n\")\n",
    "\n",
    "    print(f\"Extracted file paths saved to 'extracted_files.txt'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Minnesota data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATA_DIR = \"ghcn_us_data\"  # Directory containing all extracted CSV files\n",
    "OUTPUT_FILE = \"mn_consolidated.csv\"\n",
    "STATIONS_FILE = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt\"  # Path to the station metadata file\n",
    "os.makedirs(DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1900 files for Minnesota stations.\n",
      "Output file 'mn_consolidated.csv' already exists. Skipping consolidation.\n"
     ]
    }
   ],
   "source": [
    "# File extensions \n",
    "EXTRACTED_FILES = \"extracted_files.txt\"  \n",
    "OUTPUT_FILE = \"mn_consolidated.csv\"\n",
    "os.makedirs(\"mn_data\", exist_ok=True)\n",
    "\n",
    "# Step 1: Filter Minnesota Files from extracted_files.txt\n",
    "def get_mn_files(extracted_files):\n",
    "    # Read all file paths from extracted_files.txt\n",
    "    with open(extracted_files, \"r\") as f:\n",
    "        all_files = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    # Filter files that include 'US1MN' in their names\n",
    "    mn_files = [file for file in all_files if \"US1MN\" in file]\n",
    "    print(f\"Found {len(mn_files)} files for Minnesota stations.\")\n",
    "    return mn_files\n",
    "\n",
    "# Step 2: Consolidate Data into a Single CSV\n",
    "def consolidate_to_csv(files, output_file):\n",
    "    combined_data = []\n",
    "    for file in tqdm(files, desc=\"Consolidating data\", unit=\"file\"):\n",
    "        try:\n",
    "            # Read each CSV into a DataFrame\n",
    "            data = pd.read_csv(\n",
    "                file, header=None,\n",
    "                names=[\"ID\", \"DATE\", \"ELEMENT\", \"DATA_VALUE\", \"M_FLAG\", \"Q_FLAG\", \"S_FLAG\", \"OBS_TIME\"]\n",
    "            )\n",
    "            combined_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "    \n",
    "    # Combine all DataFrames and save to a single CSV\n",
    "    if combined_data:\n",
    "        combined_df = pd.concat(combined_data, ignore_index=True)\n",
    "        combined_df.to_csv(output_file, index=False)\n",
    "        print(f\"Data consolidated into {output_file}.\")\n",
    "    else:\n",
    "        print(\"No data to consolidate.\")\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Filter Minnesota files\n",
    "    if not os.path.exists(EXTRACTED_FILES):\n",
    "        print(f\"Error: '{EXTRACTED_FILES}' not found. Please ensure it exists.\")\n",
    "        exit(1)\n",
    "\n",
    "    mn_files = get_mn_files(EXTRACTED_FILES)\n",
    "\n",
    "    # Step 2: Consolidate into a single CSV\n",
    "    if mn_files:\n",
    "        if not os.path.exists(OUTPUT_FILE):\n",
    "            consolidate_to_csv(mn_files, OUTPUT_FILE)\n",
    "        else:\n",
    "            print(f\"Output file '{OUTPUT_FILE}' already exists. Skipping consolidation.\")\n",
    "    else:\n",
    "        print(\"No matching files found for Minnesota stations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the consolidated Minnesota CSV file and display the first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>DATE</th>\n",
       "      <th>ELEMENT</th>\n",
       "      <th>DATA_VALUE</th>\n",
       "      <th>M_FLAG</th>\n",
       "      <th>Q_FLAG</th>\n",
       "      <th>S_FLAG</th>\n",
       "      <th>OBS_TIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US1MNAA0002</td>\n",
       "      <td>20091222</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>730.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US1MNAA0002</td>\n",
       "      <td>20091223</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>715.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US1MNAA0002</td>\n",
       "      <td>20091224</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>142</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>730.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US1MNAA0002</td>\n",
       "      <td>20091225</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>165</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>730.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US1MNAA0002</td>\n",
       "      <td>20091226</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>130</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3365230</th>\n",
       "      <td>US1MNYM0004</td>\n",
       "      <td>20241116</td>\n",
       "      <td>SNOW</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3365231</th>\n",
       "      <td>US1MNYM0004</td>\n",
       "      <td>20241117</td>\n",
       "      <td>SNOW</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3365232</th>\n",
       "      <td>US1MNYM0004</td>\n",
       "      <td>20241118</td>\n",
       "      <td>SNOW</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3365233</th>\n",
       "      <td>US1MNYM0004</td>\n",
       "      <td>20241111</td>\n",
       "      <td>DAPR</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3365234</th>\n",
       "      <td>US1MNYM0004</td>\n",
       "      <td>20241111</td>\n",
       "      <td>MDPR</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>700.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3365235 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ID      DATE ELEMENT  DATA_VALUE M_FLAG Q_FLAG S_FLAG  \\\n",
       "0        US1MNAA0002  20091222    PRCP          25    NaN    NaN      N   \n",
       "1        US1MNAA0002  20091223    PRCP           0    NaN    NaN      N   \n",
       "2        US1MNAA0002  20091224    PRCP         142    NaN    NaN      N   \n",
       "3        US1MNAA0002  20091225    PRCP         165    NaN    NaN      N   \n",
       "4        US1MNAA0002  20091226    PRCP         130    NaN    NaN      N   \n",
       "...              ...       ...     ...         ...    ...    ...    ...   \n",
       "3365230  US1MNYM0004  20241116    SNOW           0    NaN    NaN      N   \n",
       "3365231  US1MNYM0004  20241117    SNOW           0    NaN    NaN      N   \n",
       "3365232  US1MNYM0004  20241118    SNOW           0    NaN    NaN      N   \n",
       "3365233  US1MNYM0004  20241111    DAPR           3    NaN    NaN      N   \n",
       "3365234  US1MNYM0004  20241111    MDPR          15    NaN    NaN      N   \n",
       "\n",
       "         OBS_TIME  \n",
       "0           730.0  \n",
       "1           715.0  \n",
       "2           730.0  \n",
       "3           730.0  \n",
       "4           700.0  \n",
       "...           ...  \n",
       "3365230     700.0  \n",
       "3365231     700.0  \n",
       "3365232     700.0  \n",
       "3365233     700.0  \n",
       "3365234     700.0  \n",
       "\n",
       "[3365235 rows x 8 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Minnesota Data\n",
    "mn_data = pd.read_csv(\"mn_consolidated.csv\")\n",
    "mn_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. **Exploratory Data Analysis:**  \n",
    "\n",
    "- Air temperature\n",
    "- Precipitation\n",
    "- Groundwater level \n",
    "\n",
    "   - Visualizing historical trends in air temperature, precipitation, and groundwater levels.  \n",
    "   - Identifying seasonal patterns and anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. **Statistical Analysis:** \n",
    "\n",
    "\n",
    "   - Applying statistical methods to investigate climate anomalies.  \n",
    "   - Comparing long-term trends across the selected time periods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. **Results and Discussion:**  \n",
    "\n",
    "   - Highlighting significant findings from the analysis.  \n",
    "   - Discussing the implications of identified climate anomalies on the selected region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. **Conclusion:**  \n",
    "   - Summarizing key insights and potential areas for further study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
