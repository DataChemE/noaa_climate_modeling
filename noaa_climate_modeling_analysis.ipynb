{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTSA 5741 Final Project: Modeling Climate Anomalies with Statistical Analysis\n",
    " \n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This Jupyter Notebook presents the final project for DTSA 5741, focused on analyzing climate anomalies using statistical methods. The primary objective of this project is to explore and analyze historical climate data for a selected region within the United States, leveraging publicly available datasets to uncover trends and anomalies in air temperature, precipitation, and groundwater levels.\n",
    "\n",
    "For this analysis, I selected Minnesota as the study area. This region is notable for its cold weather and diverse climate patterns, making it an interesting case study for climate analysis. By examining historical climate data for Minnesota, we aim to identify any significant anomalies or trends that may have occurred over the past few decades.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "1. **[NOAA Climate Data Online Portal](https://www.ncei.noaa.gov/cdo-web/):**\n",
    "   - Provides access to a wide range of historical weather and climate data from observation stations across the United States.\n",
    "   - Used to retrieve data on air temperature and precipitation for the selected region.\n",
    "\n",
    "2. **[USGS National Water Dashboard](https://dashboard.waterdata.usgs.gov/):**\n",
    "   - Offers real-time data and trends related to water resources such as groundwater levels and streamflow.\n",
    "   - Supplemented NOAA data with hydrological information for the selected region.\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "The goals of this analysis include:  \n",
    "1. **Data Acquisition:** Importing and preprocessing datasets to ensure consistency and usability.  \n",
    "2. **Exploratory Data Analysis (EDA):** Understanding historical trends in climate variables and identifying any anomalies.  \n",
    "3. **Statistical Analysis:** Performing in-depth analysis to interpret patterns in air temperature, precipitation, and groundwater levels.  \n",
    "4. **Visualization:** Presenting findings through clear and insightful visualizations to support conclusions.  \n",
    "5. **Reporting:** Documenting the process and results in a structured and reproducible manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Structure\n",
    "\n",
    "1. **Data Import and Cleaning:**  \n",
    "   - Importing datasets \n",
    "   - Cleaning and wrangling data to address missing values, inconsistencies, or outliers.\n",
    "\n",
    "2. **Exploratory Data Analysis:**  \n",
    "   - Visualizing historical trends in air temperature, precipitation, and groundwater levels.  \n",
    "   - Identifying seasonal patterns and anomalies.\n",
    "\n",
    "3. **Statistical Analysis:**  \n",
    "   - Applying statistical methods to investigate climate anomalies.  \n",
    "   - Comparing long-term trends across the selected time periods.\n",
    "\n",
    "4. **Results and Discussion:**  \n",
    "   - Highlighting significant findings from the analysis.  \n",
    "   - Discussing the implications of identified climate anomalies on the selected region.\n",
    "\n",
    "5. **Conclusion:**  \n",
    "   - Summarizing key insights and potential areas for further study.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Project Imports\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import gzip\n",
    "import shutil\n",
    "import datetime\n",
    "import time \n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# NWIS API Call Import\n",
    "import dataretrieval.nwis as nwis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. **Data Import and Cleaning:**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Global Historical Climatology Network-Daily (GHCN-Daily) is an extensive database that compiles daily climate records from over 100,000 land-based stations worldwide. It encompasses more than 40 meteorological elements, including daily maximum and minimum temperatures, precipitation, snowfall, snow depth, evaporation, wind movement, soil temperature, and cloudiness. The dataset aggregates approximately 1.4 billion data values, with records dating back to the 19th century. Where feasible, station records are updated daily and are typically accessible one to two days after observation.\n",
    "\n",
    "Users can access the data in various formats:\n",
    "\t1.\tGHCN-Daily Form (PDF): Provides five core values and, when available, additional elements such as temperature at observation time, evaporation, 24-hour wind movement, and soil temperatures. Units are presented in either metric or standard, based on user preference.\n",
    "\t2.\tCustom GHCN-Daily CSV: Offers data optimized for spreadsheet use, allowing users to select preferred units, include flags, station names, geographic locations, and specify desired elements.\n",
    "\t3.\tCustom GHCN-Daily ASCII: Delivers data in ASCII text format with options similar to the CSV format, enabling customization of included information.\n",
    "\n",
    "The GHCN-Daily dataset serves as a replacement for older datasets maintained by the National Climatic Data Center (NCDC) and functions as the official archive for daily data from the Global Climate Observing System (GCOS) Surface Network. It is particularly suited for monitoring and assessing the frequency and magnitude of climate extremes.\n",
    "\n",
    "Some data are sourced under the World Meteorological Organization’s World Weather Watch Program, adhering to WMO Resolution 40 (Cg-XII). This permits member countries to impose restrictions on the commercial use or re-export of their data outside the receiving country. Consequently, data summaries and products are intended for unrestricted use in research, education, and other non-commercial activities. For non-U.S. locations, data or any derived products should not be provided to other users or used for the re-export of commercial services.\n",
    "\n",
    "For detailed information on data formats, observation definitions, and access methods, please refer to the GHCN-Daily documentation. ￼\n",
    "\n",
    "https://www.ncei.noaa.gov/cdo-web/datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/\n",
    "\n",
    "\n",
    "The \"station\".csv files contain all daily elements for that GHCN station for its entire period of record. \n",
    "Each element-day is provided on a separate line and all files are updated daily for the entire period of record.\n",
    "\n",
    "The following information serves as a definition of each field for all element-day records. \n",
    "Each field described below is separated by a comma ( , ) and follows the order below:\n",
    "\n",
    "ID = 11 character station identification code\n",
    "YEAR/MONTH/DAY = 8 character date in YYYYMMDD format (e.g. 19860529 = May 29, 1986)\n",
    "ELEMENT = 4 character indicator of element type \n",
    "DATA VALUE = 5 character data value for ELEMENT \n",
    "M-FLAG = 1 character Measurement Flag \n",
    "Q-FLAG = 1 character Quality Flag \n",
    "S-FLAG = 1 character Source Flag \n",
    "OBS-TIME = 4-character time of observation in hour-minute format (i.e. 0700 =7:00 am); if no ob time information \n",
    "is available, the field is left empty\n",
    "\n",
    "See section III of the GHCN-Daily readme.txt file (ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt)\n",
    "for an explanation of ELEMENT codes and their units as well as the M-FLAG, Q-FLAG and S-FLAG.\n",
    "\n",
    "The OBS-TIME field is populated with the observation times contained in NOAA/NCEI's HOMR station history database.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the data from the following link: https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOAA Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL for the dataset\n",
    "BASE_URL = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/\"\n",
    "\n",
    "# Directory to save files\n",
    "DATA_DIR = \"ghcn_us_data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Output file\n",
    "OUTPUT_FILE = \"ghcn_daily_combined.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the html of the webpage to find the links to the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML Response:\n",
      "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\n",
      "<html>\n",
      " <head>\n",
      "  <title>Index of /pub/data/ghcn/daily/by_station</title>\n",
      " </head>\n",
      " <body>\n",
      "<h1>Index of /pub/data/ghcn/daily/by_station</h1>\n",
      "  <table>\n",
      "   <tr><th><a href=\"?C=N;O=D\">Name</a></th><th><a href=\"?C=M;O=A\">Last modified</a></th><th><a href=\"?C=S;O=A\">Size</a></th><th><a href=\"?C=D;O=A\">Description</a></th></tr>\n",
      "   <tr><th colspan=\"4\"><hr></th></tr>\n",
      "<tr><td><a href=\"/pub/data/ghcn/daily/\">Parent Directory</a></td><td>&nbsp;</td><td align=\"right\">  - </td><td>&nbsp;</td></tr>\n",
      "<tr><td><a href=\"ACW00011604.csv.gz\">ACW00011604.csv.gz</a></td><td align=\"right\">2024-12-09 07:28  </td><td align=\"right\">3.9K</td><td>&nbsp;</td></tr>\n",
      "<tr><td><a href=\"ACW00011647.csv.gz\">ACW00011647.csv.gz</a></td><td align=\"right\">2024-12-09 07:28  </td><td align=\"right\"> 39K</td><td>&nbsp;</td></tr>\n",
      "<tr><td><a href=\"AE000041196.csv.gz\">AE000041196.csv.gz</a></td><td align=\"right\">2024-12-09 07:28  </td><td align=\"right\">250K</td><td>&nbsp;</td></\n"
     ]
    }
   ],
   "source": [
    "# Find the correct html references for the requests to download the data\n",
    "def fetch_and_inspect():\n",
    "    response = requests.get(BASE_URL)\n",
    "    response.raise_for_status()\n",
    "    print(\"HTML Response:\")\n",
    "    print(response.text[:1000]) \n",
    "\n",
    "fetch_and_inspect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on above the data is in the  'a' tag and the data is in the form of csv files. The data is extracted from the csv files and the data is cleaned and stored in the form of dataframes. The data is then used for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for downloading the daily NOAA data for the US sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worker function to download and extract a single file\n",
    "def process_file(us_file):\n",
    "    file_url = BASE_URL + us_file\n",
    "    file_path = os.path.join(DATA_DIR, us_file)\n",
    "    extracted_path = file_path.replace(\".gz\", \"\")\n",
    "\n",
    "    # Download the file if not already downloaded\n",
    "    if not os.path.exists(file_path):\n",
    "        try:\n",
    "            response = requests.get(file_url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            with open(file_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "        except Exception as e:\n",
    "            return f\"Error downloading {file_url}: {e}\"\n",
    "\n",
    "    # Extract the file\n",
    "    if not os.path.exists(extracted_path):\n",
    "        try:\n",
    "            with gzip.open(file_path, 'rb') as f_in:\n",
    "                with open(extracted_path, 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "        except Exception as e:\n",
    "            return f\"Error extracting {file_path}: {e}\"\n",
    "\n",
    "    return extracted_path\n",
    "\n",
    "\n",
    "# Download and extract files with parallelization\n",
    "def download_and_extract_us_files():\n",
    "    # Fetch the directory listing\n",
    "    response = requests.get(BASE_URL)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Parse the HTML response\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all anchor tags\n",
    "    links = soup.find_all('a')\n",
    "\n",
    "    # Extract and filter links ending with \".csv.gz\"\n",
    "    us_files = [link.get('href') for link in links if link.get('href') and link.get('href').startswith('US') and link.get('href').endswith('.csv.gz')]\n",
    "\n",
    "    print(f\"Found {len(us_files)} files to process.\")\n",
    "\n",
    "    extracted_files = []\n",
    "    errors = []\n",
    "\n",
    "    # Use ThreadPoolExecutor for parallel processing\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor: \n",
    "        futures = {executor.submit(process_file, us_file): us_file for us_file in us_files}\n",
    "\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing files\", unit=\"file\"):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if isinstance(result, str) and result.startswith(\"Error\"):\n",
    "                    errors.append(result)\n",
    "                else:\n",
    "                    extracted_files.append(result)\n",
    "            except Exception as e:\n",
    "                errors.append(f\"Unexpected error: {e}\")\n",
    "\n",
    "    print(f\"Processed {len(extracted_files)} files successfully.\")\n",
    "    print(f\"Encountered {len(errors)} errors.\")\n",
    "    \n",
    "    # Log errors \n",
    "    if errors:\n",
    "        with open(\"error_log.txt\", \"w\") as log_file:\n",
    "            for error in errors:\n",
    "                log_file.write(error + \"\\n\")\n",
    "        print(\"Errors logged to 'error_log.txt'.\")\n",
    "\n",
    "    return extracted_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for consolidating data from multiple files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the data in one csv file \n",
    "def consolidate_to_csv(files, output_file, batch_size=1000):\n",
    "    # Open the output file for writing\n",
    "    with open(output_file, \"w\") as f_out:\n",
    "        # Use tqdm for an overall progress bar\n",
    "        with tqdm(total=len(files), desc=\"Consolidating files\", unit=\"file\") as pbar:\n",
    "            for i in range(0, len(files), batch_size):\n",
    "                batch_files = files[i : i + batch_size]\n",
    "                combined_data = []\n",
    "                for file in batch_files:\n",
    "                    try:\n",
    "                        # Read data into a DataFrame\n",
    "                        data = pd.read_csv(\n",
    "                            file,\n",
    "                            header=None,\n",
    "                            names=[\"ID\", \"DATE\", \"ELEMENT\", \"DATA_VALUE\", \"M_FLAG\", \"Q_FLAG\", \"S_FLAG\", \"OBS_TIME\"],\n",
    "                            dtype={\n",
    "                                \"ID\": str,\n",
    "                                \"DATE\": str,\n",
    "                                \"ELEMENT\": str,\n",
    "                                \"DATA_VALUE\": \"float64\",\n",
    "                                \"M_FLAG\": str,\n",
    "                                \"Q_FLAG\": str,\n",
    "                                \"S_FLAG\": str,\n",
    "                                \"OBS_TIME\": str,\n",
    "                            },\n",
    "                        )\n",
    "                        combined_data.append(data)\n",
    "                    except Exception as e:\n",
    "                        pbar.write(f\"Error processing {file}: {e}\")\n",
    "                    finally:\n",
    "                        pbar.update(1)\n",
    "\n",
    "                # Concatenate and write the batch to the output file\n",
    "                if combined_data:\n",
    "                    batch_df = pd.concat(combined_data, ignore_index=True)\n",
    "                    batch_df.to_csv(f_out, index=False, header=(i == 0), mode=\"a\")  # Write header only for the first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and Extract File paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and extracting files...\n",
      "Found 74429 files to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 74429/74429 [00:01<00:00, 52603.01file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 74429 files successfully.\n",
      "Encountered 0 errors.\n",
      "Extracted file paths saved to 'extracted_files.txt'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Download and extract files\n",
    "    print(\"Downloading and extracting files...\")\n",
    "    extracted_files = download_and_extract_us_files()\n",
    "\n",
    "    # Save the list of extracted files to a text file\n",
    "    with open(\"extracted_files.txt\", \"w\") as f:\n",
    "        for file in extracted_files:\n",
    "            f.write(file + \"\\n\")\n",
    "\n",
    "    print(f\"Extracted file paths saved to 'extracted_files.txt'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consolidate Data into a Single CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading extracted file paths...\n",
      "Found 74429 files to consolidate.\n",
      "Consolidating data into a single CSV file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Consolidating files: 100%|██████████| 74429/74429 [56:54<00:00, 21.80file/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data consolidated into 'ghcn_daily_combined.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Add data to one csv file\n",
    "if __name__ == \"__main__\":\n",
    "    # Consolidate data into a single CSV\n",
    "    if os.path.exists(OUTPUT_FILE):\n",
    "        print(f\"Output file '{OUTPUT_FILE}' already exists. Skipping consolidation.\")\n",
    "    else:\n",
    "        print(\"Reading extracted file paths...\")\n",
    "        try:\n",
    "            with open(\"extracted_files.txt\", \"r\") as f:\n",
    "                extracted_files = [line.strip() for line in f.readlines()]\n",
    "        except FileNotFoundError:\n",
    "            print(\"Error: 'extracted_files.txt' not found. Please run Step 1 first.\")\n",
    "            exit(1)\n",
    "\n",
    "        print(f\"Found {len(extracted_files)} files to consolidate.\")\n",
    "        print(\"Consolidating data into a single CSV file...\")\n",
    "\n",
    "        \n",
    "        consolidate_to_csv(extracted_files, OUTPUT_FILE)\n",
    "\n",
    "        print(f\"Data consolidated into '{OUTPUT_FILE}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a database to pull from for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep credentials in .env file\n",
    "load_dotenv()\n",
    "file_path = \"./ghcn_daily_combined.csv\"\n",
    "# Create a connection to the PostgreSQL database\n",
    "DATABASE_URI = f\"postgresql://{os.environ['PG_USER']}:{os.environ['PG_PASS']}@localhost/us_data\"\n",
    "engine = create_engine(DATABASE_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Database Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "# Function to create the table\n",
    "def create_table():\n",
    "    create_table_query = text(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS large_table (\n",
    "        ID TEXT,\n",
    "        DATE TEXT,\n",
    "        ELEMENT TEXT,\n",
    "        DATA_VALUE DOUBLE PRECISION,\n",
    "        M_FLAG TEXT,\n",
    "        Q_FLAG TEXT,\n",
    "        S_FLAG TEXT,\n",
    "        OBS_TIME TEXT\n",
    "    );\n",
    "    \"\"\")\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(create_table_query)\n",
    "        print(\"Table 'large_table' created or already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import into postgresql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into the table\n",
    "def import_csv_to_postgres(file_path):\n",
    "    # Read and load in chunks to handle large files\n",
    "    chunk_size = 1_000_000\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "        try:\n",
    "            # Append data to the table\n",
    "            chunk.to_sql(\"large_table\", engine, if_exists=\"append\", index=False)\n",
    "            print(f\"Loaded a chunk of {len(chunk)} rows.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute the creation of the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'large_table' created or already exists.\n",
      "Loaded a chunk of 1000000 rows.\n",
      "Loaded a chunk of 1000000 rows.\n",
      "Loaded a chunk of 1000000 rows.\n",
      "Loaded a chunk of 1000000 rows.\n",
      "Loaded a chunk of 1000000 rows.\n",
      "Loaded a chunk of 1000000 rows.\n",
      "Loaded a chunk of 1000000 rows.\n",
      "Loaded a chunk of 1000000 rows.\n",
      "Loaded a chunk of 1000000 rows.\n",
      "Loaded a chunk of 1000000 rows.\n",
      "Loaded a chunk of 1000000 rows.\n",
      "Loaded a chunk of 1000000 rows.\n",
      "Loaded a chunk of 1000000 rows.\n",
      "Loaded a chunk of 1000000 rows.\n",
      "Loaded a chunk of 1000000 rows.\n",
      "Loaded a chunk of 1000000 rows.\n",
      "Loaded a chunk of 1000000 rows.\n",
      "Loaded a chunk of 1000000 rows.\n",
      "Loaded a chunk of 1000000 rows.\n",
      "Loaded a chunk of 1000000 rows.\n"
     ]
    }
   ],
   "source": [
    "# Execute the steps\n",
    "if __name__ == \"__main__\":\n",
    "    create_table()\n",
    "    import_csv_to_postgres(file_path)\n",
    "    print(\"CSV file imported successfully into PostgreSQL.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test connection\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        print(\"Connection successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. **Exploratory Data Analysis:**  \n",
    "\n",
    "- Air temperature\n",
    "- Precipitation\n",
    "- Groundwater level \n",
    "\n",
    "   - Visualizing historical trends in air temperature, precipitation, and groundwater levels.  \n",
    "   - Identifying seasonal patterns and anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. **Statistical Analysis:** \n",
    "\n",
    "\n",
    "   - Applying statistical methods to investigate climate anomalies.  \n",
    "   - Comparing long-term trends across the selected time periods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. **Results and Discussion:**  \n",
    "\n",
    "   - Highlighting significant findings from the analysis.  \n",
    "   - Discussing the implications of identified climate anomalies on the selected region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. **Conclusion:**  \n",
    "   - Summarizing key insights and potential areas for further study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
